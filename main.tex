%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template for USENIX papers.
%
% History:
%
% - TEMPLATE for Usenix papers, specifically to meet requirements of
%   USENIX '05. originally a template for producing IEEE-format
%   articles using LaTeX. written by Matthew Ward, CS Department,
%   Worcester Polytechnic Institute. adapted by David Beazley for his
%   excellent SWIG paper in Proceedings, Tcl 96. turned into a
%   smartass generic template by De Clarke, with thanks to both the
%   above pioneers. Use at your own risk. Complaints to /dev/null.
%   Make it two column with no page numbering, default is 10 point.
%
% - Munged by Fred Douglis <douglis@research.att.com> 10/97 to
%   separate the .sty file from the LaTeX source template, so that
%   people can more easily include the .sty file into an existing
%   document. Also changed to more closely follow the style guidelines
%   as represented by the Word sample file.
%
% - Note that since 2010, USENIX does not require endnotes. If you
%   want foot of page notes, don't include the endnotes package in the
%   usepackage command, below.
% - This version uses the latex2e styles, not the very ancient 2.09
%   stuff.
%
% - Updated July 2018: Text block size changed from 6.5" to 7"
%
% - Updated Dec 2018 for ATC'19:
%
%   * Revised text to pass HotCRP's auto-formatting check, with
%     hotcrp.settings.submission_form.body_font_size=10pt, and
%     hotcrp.settings.submission_form.line_height=12pt
%
%   * Switched from \endnote-s to \footnote-s to match Usenix's policy.
%
%   * \section* => \begin{abstract} ... \end{abstract}
%
%   * Make template self-contained in terms of bibtex entires, to allow
%     this file to be compiled. (And changing refs style to 'plain'.)
%
%   * Make template self-contained in terms of figures, to
%     allow this file to be compiled. 
%
%   * Added packages for hyperref, embedding fonts, and improving
%     appearance.
%   
%   * Removed outdated text.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix-2020-09}

% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf L-Diversity for Custom Data Analysis:\\
  Swapping Similar Rows}

%for single author (just remove % characters)
\author{
{\rm Sam Boger and Roberto Tamassia} \\
Department of Computer Science, Brown University

% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\maketitle
\section*{Abstract}
Data anonymization should support the analysts who intend to use the anonymized data. Releasing datasets that contain personal information requires anonymization that balances privacy conerns while preserving the utility of the data. This work shows how choosing anonymization techniques with the data analysts' requirements in mind improves effectiveness quantitatively by minimizing the discrepancy between querying the original data versus the anonymized result and qualitatively by simplifying the workflow for processing the data.

\section{Introduction}
Data sharing offers immense opportunity as modern machine learning models and human analysis often depends on large training sets. However, valuable information is frequently also personal, and privacy concerns can prevent data owners from sharing their data. Several approaches have been offered in order to alleviate privacy concerns while still sharing useful data to the public. K-anonymity reduces the chance of an individual’s data being re-identified, but it can leak too much information about an individual's data in certain adversarial models and data contexts. L-diversity can offer stronger privacy guarantees at the cost of reducing the utility of the released data. Evaluating the utility of privacy preserving data modification is itself a challenging topic--the most common analysis strategies cover table-wide metrics or machine learning model quality using the anonymized data.

Shifting perspectives to the data consumer can clarify how to choose between these options. In particular, imagine analysts know the schema of the original dataset and already know what kinds of queries they care most about. Now, utility can be best defined as minimizing the discrepancy the analysts will see between querying the original data and the anonymized data. Furthermore, changing the schema or data definitions when anonymizing the data would disrupt their workflow and should be avoided if possible.

Different prior works in this field address these design decisions separately which invites an opportunity to combine these techniques. In particular, allowing the analysts to easily customize the heuristics used in the anonymization algorithm, as well as publishing data with identical schema and semantics to the original dataset, will best meet these requirements while still protecting individuals' privacy.

%TODO: Quasi-identifiers, sensitive values

\subsection{Unique Contributions}
% MOVE to subsection of introduction
This project intends to combine the insights from these different works to describe a promising anonymization technique that adequately protects privacy, preserves utility of the dataset, and produces an easy-to-use resulting dataset. In particular, the proposed method uses a customized distance function\cite{jiaPad} to cluster, and performs data swapping\cite{soriaSwapping} within these clusters, to produce an l-diverse\cite{machanavajjhalalDiversity} anonymized dataset.

\section{Related Work}
\subsection{Anonymization Definitions}
K-anonymity\cite{sweeneykAnonymity} is one of the most influential works for defining privacy-preserving anonymization. Removing personally identifiable information from datasets can still leave data that act as quasi-identifiers (QIs) which can be combined with separate data sources by an adversary to re-identify an individual. If the original dataset has sensitive information about individuals, then this re-identification leaks the association of that individual to those sensitive values and thus the effort to remove the identifiers was insufficient to preserve privacy. K-anonmyity address this concern by ensuring that at least $k$ rows have identical QIs so that any individual effectively blends in with at least $k-1$ other individuals. To create these homogeneous groups, the QIs are mapped onto a less granular space, e.g. full birth date may be mapped instead to birth year. Furthermore, certain rows can be suppressed (removed from the dataset entirely) to reduce the amount of generalization necessary. Now, individual re-identification from outside data sources can only imply membership in a group of size at least $k$. Many subsequent works expanded on this original definition and discussed improvements to this method.

L-diversity\cite{machanavajjhalalDiversity} is one such extension to this method which places a further constraint on the anonymized dataset that each group of records with identical QIs must also satisfy. L-diversity can be used to describe a couple different properties. Here, we call a group is L-diverse if and only if every group contains at least L distinct sensitive values, and the table is L-diverse if and only if every record belong to a group that is L-diverse. Another property we will use is $\theta$-diverse. Call the size of a group of records N, and the most frequent sensitive value in the group S. The group is $\theta$-diverse if and only if $\frac{S}{N}<\theta$, and the table is $\theta$-diverse if and only if every record belong to a group that is $\theta$-diverse. Intuitively this means that learning which group an individual belongs to will never reveal its associated sensitive value with probability greater than $\theta$. Without this property, it is possible that all sensitive values for a K-anonymous group are the same, and identifying an individual within that group reveals their sensitive value regardless of the group's size. Furthermore, auxiliary information may exist that one or more sensitive value is highly unlikely for an individual, so a diverse set of sensitive values within each group protects against this process of elimination style of attack. The tunable parameters L and $\theta$ in this scheme control the likelihood of this leakage by varying the diversity of every group.

Further restrictions have also been considered, including T-closeness\cite{litCloseness} where each group of homogenized records must contain a similar (within a factor of T) distribution of sensitive values as the entire original table. This emphasizes privacy guarantees at the expense of increased difficulty in computing the anonymized table and decreased utility for analysis. For this project we do not consider T-closeness although similar ideas would apply in principle.

\subsection{Methods of Computing Anonymizations}
In order to maintain better utility of the anonymized table, as well as more efficient computation of the anonymized dataset, many papers have explored clustering methods for constructing K-anonymity or L-diversity groups rather than arbitrary groupings. By grouping together similar records, the correlations that exist in the original dataset can be better preserved while still providing the privacy guarantees of these anonymization methods\cite{niClustering}\cite{liuDensity}\cite{chiuClustering}. Different clustering algorithms are described in these works (weighted feature C-means, density clustering) which all seek to collect similar records into groups that will then be generalized to produce the anonymized datasets. Most of these methods address K-anonymity only, as efficiently computing L-diverse and $\theta$-diverse clusters presents further challenges. One algorithm for computing K-anonymous, L-diverse, and $\theta$-Diverse clusters demonstrates effective in practice but the algorithm lacks complete details and guarantees in the general case \cite{yangEnhanced} which is adapted for this work.

In addition to varying the clustering algorithms, other work has modified the distance metric between rows to better preserve utility for a particular use case\cite{jiaPad}. Here, the data analysts using the anonymized data contribute training data regarding which rows they consider similar to teach the anonymization system a customized distance metric. Clustering according to this metric still produces a K-anonymous dataset, but the groups contain rows that are more similar according to this customized notion of distance.

Another approach to preserving utility is proposed and analyzed in \cite{soriaSwapping}, where instead of producing a dataset with groups of rows that have identical quasi-identifiers through generalization and suppression, all records in a given group are randomly swapped with their associated sensitive values. Even for an adversary with knowledge of which rows were in the same group, this ensures a similar guarantee as a traditional $\theta$-diverse table where re-identifying a certain row only reveals the correct sensitive value with probability $\leq\theta$. 

Similarly to shuffling within each group of records, \cite{xiaoAnatomy} proposes publishing the sets of QIs in a separate table than the sensitive values but with a join key specific to each group--which they call an anatomy. Therefore, similarly to\cite{soriaSwapping}, the QIs in the original dataset are published but it is not revealed exactly which sensitive value corresponds to which row. This paper evaluates the increased utility of this method by measuring query accuracy.

\subsection{Utility Analysis}
There are mulitple techniques for this analysis. Most common are table-wide statistical metrics (e.g. \cite{soriaSwapping}) and the quality of machine learning model trained over the anonymized data (e.g. \cite{sangogboyePad}). Earlier work\cite{xiaoAnatomy} uses a test suite of queries to assess utility. Other efforts attempt to reconcile the different metrics but conclude that it is difficult to correlate the statistical measures of the anaonymized datasets to the effectiveness of models trained over those datasets\cite{vsarvcevicEffectiveness}.

The anatomy work\cite{xiaoAnatomy} creates tests counting queries by randomly choosing attribute values to select for at various configurations. They compute the relative error by running the same queries on a dataset with no anonymization versus L-diverse datasets produced with generalization or anatomy. Their results demonstrate that the relative error in answering these queries for anatomy is less than the relative error for generalization.

To evaluate data swapping\cite{soriaSwapping}, the authors quantify the improved utility of a data swapped and generalized tables at the aggregate level by computing and comparing the correlation between QIs and sensitive values in the different anonymized datasets compared to the original dataset. Data swapping preserves correlation much better than generalization in their experiments.

\section{Clustering Method and Data Analysis}
This section will describe the possible relationships between the anonymization’s clustering method and the queries to be run over the anonymized data. Specifically, this will introduce the notation to describe the QIs that are prioritized for clustering, the other QIs, and the sensitive values. The details depend on the implementation of clustering and the configurations that will be chosen.

\section{Experimental Setup}

\subsection{Dataset}
The dataset that most frequently occurs in prior works on this topic is the UCI ADULT database\cite{adultDatabase} which will be used here as well. This dataset contains census records with demographic, financial, and other personal information.

The experiments will use age, race, sex, and education as the quasi-identifiers and occupation as the sensitive value. These are fields defined in the dataset as coming from a set of enumerated values, with all but age being strings, and ages are integers in the range $[18,90]$.

\subsection{Privacy Implementation}
The default anonymization will be performed with K-anonymous groups of size $K=10$ meaning every group has at least 10 rows, L-diversity at $L=5$ meaning every group has at least 5 sensitive values, and $\theta=.3$ meaning no sensitive value accounts for more than $\frac{3}{10}$ of the sensitive values in each group. The sensitive values in the group are then swapped randomly among every group member.

Clustering is done with an algorithm based on prior work\cite{yangEnhanced} with some of the open-ended steps filled in based on trial and error. This algorithm is not guaranteed to produce a valid clustering in that it may fail to place a large percentage of the rows in clusters and/or run for an intracible amount of time in general. However, on this dataset and with these choices of parameters, the algorithm performs very well and satisfies these strict privacy constraints.

\subsection{Distance Metrics}
The clustering algorithm is agnostic as to the distance metric used. Here, the clusters are compared based on a weighted sum of orthogonal comparisons. For age and sex the algorithm computes the average of each weighted between 0 and 1, with the two sexes present in the dataset arbitrarily mapped to 1 and 0 respectively, and ages linearly mapped to [0,1]. These cluster averages are each then combined using weighted univariate distance as discussed in this prior work \cite{domingePracticalCluster}. The other categorical dimensions are defined to be of distance 1 if there is any value in one set that is not present in the other and 0 otherwise. The total distance between two clusters is then a linear combination of each of these distances.

The weights of the linear combination, which default to 1, impact the utility of the result with respect to certain types of queries. For instance, weighting the age distance more heavily will produce clusters that have lower variance of age within clusters. These weights will be varied over the different experiments below to demonstrate how this impacts analyzing the resulting dataset.

\subsection{Analysis}
\begin{enumerate}
    \item Naive cluster generation for l-diversity.
    \item Custom distance metrics for l-diversity.
    \item For each, their performance on a suite of queries as compared to running those queries over the full table (non-anonymized).
\end{enumerate}

\section{Attacking Anonymized Data}
Prior work on the privacy of k-anonymity addresses privacy risks in different ways. The original papers make reasonable assumptions about the types of background knowledge an adversary might have when doing formal calculations about probabilities of a successful attack. They note that in principle it is impossible to account for arbitrary background knowledge of attackers.

Subsequent works advance the theory surrounding this topic by bringing in techniques from other areas of computer science. Information theory can provide a framework for considering the tension in anonymization between providing accurate information to the legitimate users of the anonymized dataset while withholding private information from adversaries with access to the same dataset\cite{liuInfoTheory}. With a more sophisticated model of this tradeoff, the problem can be viewed as an optimization of this tradeoff for average or worst-case information leakage to an adversary\cite{duInfoTheory}.

For any particular k-anonymous dataset, it is possible to launch a customized attack with background knowledge to attempt to recover information about the original dataset. One such example \cite{schweeEval} was able to reidentify particular rooms in a k-anonymous dataset unless the anonymization also employed suppression to remove outliers in the data.

\section{Privacy Properties}
The primary privacy property of these methods is to reduce the likelihood of an adversary associating an  entity to its sensitive value. Prior work on K-anonymity, L-diversity, and $\theta$-diversity assume that an adversary can still reidentify a row within a group, but hides the sensitive value among the other members of the group.

This solution further assumes that although the output does not indicate which group a row belongs to, that the adversary can still figure this out. In an extreme case where the clusters are disjoint across a particular attribute like age, and the adversary knows the age of their target, this assumption is plausible. In a typical case, it is at least an extra layer of obscurity that the groups are not explicitly revealed, but the privacy properties hold in any case.

By ensuring that every group meets the requirements of K,L,and $\theta$ diversity, this solution achieves the privacy properties of each.

[Clustering along a dimension reduces implausible sensitive values]

Another desired privacy property could be to prevent an adversary form learning whether an entity is present in the dataset at all, regardless of their sensitive value. This is a fundamental weakness of both the anatomy and data swapping approaches, since the full set of quasi-identifiers are present in the anonymized dataset, giving the adversary a very good chance of succeeding in such a membership test. However, as mentioned, none of these methods fully address membership testing as even a K,L, and $\theta$-anonymized dataset using generalization could still be vulnerable to this type of attack.

\section{Conclusion}


%-------------------------------------------------------------------------------
\bibliographystyle{plain}

% \bibliography{\jobname}

\bibliography{bib/references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks